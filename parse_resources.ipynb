{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse ArchivesSpace Resources via the API\n",
    "\n",
    "### Import packages\n",
    "- configparser: Implements a basic configuration language which provides a structure you can use to write Python programs which can be customized by end users.\n",
    "- json: Exposes an API for JSON (JavaScript Object Notation).\n",
    "- requests: A HTTP library.\n",
    "- pandas: An open source data analysis and manipulation tool, built on top of the Python programming language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Configuration File\n",
    "\n",
    "In order to authenticate to ArchivesSpace and thus use the API, you'll have needed to supply a separate -- and ignored by git -- \"config.ini\" file in the home directory that looks like this:\n",
    "\n",
    "```\n",
    "[ARCHIVESSPACE]\n",
    "BaseURL = \n",
    "User = \n",
    "Password = \n",
    "Respository ID = \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Reading Configuration File')\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "base_url = config['ARCHIVESSPACE']['BaseURL']\n",
    "user = config['ARCHIVESSPACE']['User']\n",
    "password = config['ARCHIVESSPACE']['Password']\n",
    "repository_id = config['ARCHIVESSPACE']['RepositoryID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authenticate to ArchivesSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Authenticating to ArchivesSpace')\n",
    "endpoint = '/users/' + user + '/login'\n",
    "params = {'password': password}\n",
    "response = requests.post(base_url + endpoint, params=params)\n",
    "print(response.status_code)\n",
    "\n",
    "response = response.json()\n",
    "session_key = response['session']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Resource IDs\n",
    "\n",
    "You can either get a list of _all_ resource IDs an an ArchivesSpace Repository, or you supply a separate \"resource_ids.txt\" file in the home directory with one line for every Resource ID for every Resource you want to parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_ids = []\n",
    "\n",
    "value = input('I want to parse all Resource IDs in ArchivesSpace. Enter True of False: ') or 'False'\n",
    "\n",
    "# Convert the input to a boolean value\n",
    "if value.lower() == \"true\":\n",
    "    bool_value = True\n",
    "elif value.lower() == \"false\":\n",
    "    bool_value = False\n",
    "else:\n",
    "    print(\"Invalid input. Please enter True or False.\")\n",
    "\n",
    "# Use the boolean value\n",
    "if bool_value:\n",
    "    print('Parsing all Resource Ids in ArchivesSpace.')\n",
    "    print('  - GETing Resource IDs')\n",
    "    endpoint = '/repositories/' + str(repository_id) + '/resources'\n",
    "    headers = {'X-ArchivesSpace-Session': session_key}\n",
    "    params = {'all_ids': True}\n",
    "    response = requests.get(base_url + endpoint, headers=headers, params=params)\n",
    "    print(response.status_code)\n",
    "\n",
    "    resource_ids = response.json()\n",
    "\n",
    "else:\n",
    "    print('Parsing Resource IDs from text file.')\n",
    "    with open('resource_ids-nativeAmerican.txt', mode='r') as f:\n",
    "    # with open('resource_ids-philippines.txt', mode='r') as f:\n",
    "        resource_ids = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for resource_id in resource_ids:\n",
    "    resource_id = str(resource_id).strip()\n",
    "\n",
    "    print('  - GETing Resource ' + str(resource_id))\n",
    "    endpoint = '/repositories/' + str(repository_id) + '/resources/' + str(resource_id)\n",
    "    headers = {'X-ArchivesSpace-Session': session_key}\n",
    "    response = requests.get(base_url + endpoint, headers=headers)\n",
    "    print(response.status_code)\n",
    "\n",
    "    resource = response.json()\n",
    "    \n",
    "    if resource['publish'] == True:\n",
    "\n",
    "        ## extract id\n",
    "        eadid = resource.get('ead_id', 'No EAD ID') # \"Stubs\" in ArchivesSpace, i.e., resources that were created from MARC data, have no EAD ID\n",
    "\n",
    "        # Extract titleproper\n",
    "        titleproper = resource['finding_aid_title'][16:]\n",
    "\n",
    "        ## extract abstract\n",
    "        abstract = ''\n",
    "        for note in resource['notes']:\n",
    "            if note.get('type') == 'abstract':\n",
    "                abstract = note['content'][0]\n",
    "                    \n",
    "        ## Extract language\n",
    "        language = resource.get('finding_aid_language_note'.replace('<language encodinganalog=\"Language\" langcode=\"eng\">English.</language>', 'English.'), 'No Language Note') # \"Stubs\" in ArchivesSpace, i.e., resources that were created from MARC data, have no Language Note, or at least not consistently\n",
    "\n",
    "        ## Extract scopecontent\n",
    "        scopecontent = ''\n",
    "        for note in resource['notes']:\n",
    "            if note.get('type') == 'scopecontent':\n",
    "                scopecontent = note['subnotes'][0]['content']\n",
    "                    \n",
    "        ## Extract bioghist    \n",
    "        bioghist = ''\n",
    "        for note in resource['notes']:\n",
    "            if note.get('type') == 'bioghist':\n",
    "                bioghist = note['subnotes'][0].get('content', '')\n",
    "                    \n",
    "        ## Extract custodhist   \n",
    "\n",
    "        ## Extract controlaccess\n",
    "        subject_ids = []\n",
    "        subjects = []\n",
    "        subjects_source = []\n",
    "\n",
    "        genreform_ids = []\n",
    "        genreforms = []\n",
    "        genreforms_source = []\n",
    "\n",
    "        geogname_ids = []\n",
    "        geognames = []\n",
    "        geognames_source = []\n",
    "\n",
    "        for subject in resource['subjects']:\n",
    "            subject_id = subject['ref'].split('/')[-1]\n",
    "                \n",
    "            print('  - GETing Subject ' + str(subject_id))\n",
    "            endpoint = '/subjects/' + str(subject_id)\n",
    "            response = requests.get(base_url + endpoint, headers=headers)\n",
    "            print(response.status_code)\n",
    "                \n",
    "            subject = response.json()\n",
    "                \n",
    "            if subject['terms'][0]['term_type'] == 'topical':\n",
    "                subject_ids.append(subject_id)\n",
    "                subjects.append(subject['terms'][0]['term'])\n",
    "                subjects_source.append(subject.get('source', 'No Source'))\n",
    "                \n",
    "            if subject['terms'][0]['term_type'] == 'genre_form':\n",
    "                genreform_ids.append(subject_id)\n",
    "                genreforms.append(subject['terms'][0]['term'])\n",
    "                genreforms_source.append(subject.get('source', 'No Source'))\n",
    "                \n",
    "            if subject['terms'][0]['term_type'] == 'geographic':\n",
    "                geogname_ids.append(subject_id)\n",
    "                geognames.append(subject['terms'][0]['term'])\n",
    "                geognames_source.append(subject.get('source', 'No Source'))\n",
    "\n",
    "        persname_ids = []\n",
    "        persnames = []\n",
    "        persnames_source = []\n",
    "\n",
    "        corpname_ids = []\n",
    "        corpnames = []\n",
    "        corpnames_source = []\n",
    "\n",
    "        famname_ids = []\n",
    "        famnames = []\n",
    "        famnames_source = []\n",
    "\n",
    "        for linked_agent in resource['linked_agents']:\n",
    "            linked_agent_id = linked_agent['ref'].split('/')[-1]\n",
    "                \n",
    "            if 'people' in linked_agent['ref']:\n",
    "                print('  - GETing Person Agent ' + str(linked_agent_id))\n",
    "                endpoint = '/agents/people/' + str(linked_agent_id)\n",
    "                response = requests.get(base_url + endpoint, headers=headers)\n",
    "                print(response.status_code)\n",
    "                \n",
    "                person_agent = response.json()\n",
    "                persname_ids.append(linked_agent_id)\n",
    "                persnames.append(person_agent['names'][0]['sort_name'])\n",
    "                persnames_source.append(person_agent['names'][0].get('source', 'No Source'))\n",
    "                    \n",
    "            if 'corporate_entities' in linked_agent['ref']:\n",
    "                print('  - GETing Coporate Entity Agent ' + str(linked_agent_id))\n",
    "                endpoint = '/agents/corporate_entities/' + str(linked_agent_id)\n",
    "                response = requests.get(base_url + endpoint, headers=headers)\n",
    "                print(response.status_code)\n",
    "                \n",
    "                corporate_entity_agent = response.json()\n",
    "                corpname_ids.append(linked_agent_id)\n",
    "                corpnames.append(corporate_entity_agent['names'][0]['sort_name'])\n",
    "                corpnames_source.append(corporate_entity_agent['names'][0].get('source', 'No Source'))\n",
    "                    \n",
    "            if 'families' in linked_agent['ref']:\n",
    "                print('  - GETing Family Agent ' + str(linked_agent_id))\n",
    "                endpoint = '/agents/families/' + str(linked_agent_id)\n",
    "                response = requests.get(base_url + endpoint, headers=headers)\n",
    "                print(response.status_code)\n",
    "                \n",
    "                family_agent = response.json()\n",
    "                famname_ids.append(linked_agent_id)\n",
    "                famnames.append(family_agent['names'][0]['sort_name'])\n",
    "                famnames_source.append(family_agent['names'][0].get('source', 'No Source'))\n",
    "                    \n",
    "        result = [str(resource_id), \n",
    "                eadid, \n",
    "                titleproper, \n",
    "                abstract, \n",
    "                language, \n",
    "                scopecontent, \n",
    "                bioghist, \n",
    "                '; '.join(subject_ids), \n",
    "                '; '.join(subjects), \n",
    "                '; '.join(subjects_source), \n",
    "                '; '.join(genreform_ids), \n",
    "                '; '.join(genreforms), \n",
    "                '; '.join(genreforms_source), \n",
    "                '; '.join(geogname_ids), \n",
    "                '; '.join(geognames), \n",
    "                '; '.join(geognames_source), \n",
    "                '; '.join(persname_ids), \n",
    "                '; '.join(persnames), \n",
    "                '; '.join(persnames_source), \n",
    "                '; '.join(corpname_ids), \n",
    "                '; '.join(corpnames), \n",
    "                '; '.join(corpnames_source), \n",
    "                '; '.join(famname_ids), \n",
    "                '; '.join(famnames), \n",
    "                '; '.join(famnames_source)]\n",
    "        results.append(result)\n",
    "\n",
    "    # Create the pandas DataFrame \n",
    "    df = pd.DataFrame(results, columns = ['resource_id',\n",
    "                                        'ead_id', \n",
    "                                        'titleproper', \n",
    "                                        'abstract', \n",
    "                                        'language', \n",
    "                                        'scopecontent', \n",
    "                                        'bioghist', \n",
    "                                        'subject_ids',\n",
    "                                        'subjects', \n",
    "                                        'subjects_source', \n",
    "                                        'genreform_ids',\n",
    "                                        'genreforms', \n",
    "                                        'genreforms_source', \n",
    "                                        'geogname_ids',\n",
    "                                        'geognames', \n",
    "                                        'geognames_source', \n",
    "                                        'persname_ids',\n",
    "                                        'persnames', \n",
    "                                        'persnames_source', \n",
    "                                        'corpname_ids',\n",
    "                                        'corpnames', \n",
    "                                        'corpnames_source', \n",
    "                                        'famname_ids',\n",
    "                                        'famnames', \n",
    "                                        'famnames_source']) \n",
    "\n",
    "print(\"Alright, we're done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Results to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Writing Results to CSV file')\n",
    "\n",
    "if bool_value:\n",
    "    df.to_csv(os.path.join('data', 'data-allIDs.csv'), encoding='utf-8', index=False)\n",
    "else:\n",
    "    df.to_csv(os.path.join('data', 'data-nativeAmerican.csv'), encoding='utf-8', index=False)\n",
    "    # df.to_csv(os.path.join('data', 'results-philippines.csv'), encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
